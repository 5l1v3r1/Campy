//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-22781540
// Cuda compilation tools, release 9.0, V9.0.176
// Based on LLVM 3.4svn
//

.version 6.0
.target sm_35
.address_size 64

	// .weak	cudaMalloc
.extern .func  (.param .b64 func_retval0) _Z7GstrlenPKc
(
	.param .b64 _Z7GstrlenPKc_param_0
)
;
.extern .func  (.param .b64 func_retval0) _Z7Gmallocy
(
	.param .b64 _Z7Gmallocy_param_0
)
;
.visible .global .align 8 .u64 ___strtok;

.weak .func  (.param .b32 func_retval0) cudaMalloc(
	.param .b64 cudaMalloc_param_0,
	.param .b64 cudaMalloc_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaFuncGetAttributes
.weak .func  (.param .b32 func_retval0) cudaFuncGetAttributes(
	.param .b64 cudaFuncGetAttributes_param_0,
	.param .b64 cudaFuncGetAttributes_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaDeviceGetAttribute
.weak .func  (.param .b32 func_retval0) cudaDeviceGetAttribute(
	.param .b64 cudaDeviceGetAttribute_param_0,
	.param .b32 cudaDeviceGetAttribute_param_1,
	.param .b32 cudaDeviceGetAttribute_param_2
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaGetDevice
.weak .func  (.param .b32 func_retval0) cudaGetDevice(
	.param .b64 cudaGetDevice_param_0
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaOccupancyMaxActiveBlocksPerMultiprocessor
.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessor(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessor_param_3
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .weak	cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
.weak .func  (.param .b32 func_retval0) cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_0,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_1,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_2,
	.param .b64 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_3,
	.param .b32 cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags_param_4
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	// .globl	_Z12GstrncasecmpPKcS0_y
.visible .func  (.param .b32 func_retval0) _Z12GstrncasecmpPKcS0_y(
	.param .b64 _Z12GstrncasecmpPKcS0_y_param_0,
	.param .b64 _Z12GstrncasecmpPKcS0_y_param_1,
	.param .b64 _Z12GstrncasecmpPKcS0_y_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<21>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd12, [_Z12GstrncasecmpPKcS0_y_param_0];
	ld.param.u64 	%rd11, [_Z12GstrncasecmpPKcS0_y_param_1];
	ld.param.u64 	%rd10, [_Z12GstrncasecmpPKcS0_y_param_2];
	setp.eq.s64	%p1, %rd10, 0;
	mov.u16 	%rs19, 0;
	mov.u16 	%rs20, %rs19;
	@%p1 bra 	BB6_5;

BB6_1:
	ld.u8 	%rs20, [%rd11];
	ld.u8 	%rs19, [%rd12];
	setp.eq.s16	%p2, %rs19, 0;
	setp.eq.s16	%p3, %rs20, 0;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB6_5;

	setp.eq.s16	%p5, %rs19, %rs20;
	@%p5 bra 	BB6_4;

	cvt.u32.u16	%r1, %rs20;
	add.s16 	%rs10, %rs19, -65;
	and.b16  	%rs11, %rs10, 255;
	setp.lt.u16	%p6, %rs11, 26;
	cvt.u32.u16	%r2, %rs19;
	add.s32 	%r3, %r2, 32;
	cvt.u16.u32	%rs12, %r3;
	selp.b16	%rs19, %rs12, %rs19, %p6;
	and.b16  	%rs13, %rs19, 255;
	add.s16 	%rs14, %rs20, -65;
	and.b16  	%rs15, %rs14, 255;
	setp.lt.u16	%p7, %rs15, 26;
	add.s32 	%r4, %r1, 32;
	cvt.u16.u32	%rs16, %r4;
	selp.b16	%rs20, %rs16, %rs20, %p7;
	and.b16  	%rs17, %rs20, 255;
	setp.ne.s16	%p8, %rs13, %rs17;
	@%p8 bra 	BB6_5;

BB6_4:
	add.s64 	%rd12, %rd12, 1;
	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, -1;
	setp.ne.s64	%p9, %rd10, 0;
	mov.u16 	%rs20, %rs19;
	@%p9 bra 	BB6_1;

BB6_5:
	cvt.u32.u16	%r5, %rs20;
	and.b32  	%r6, %r5, 255;
	cvt.u32.u16	%r7, %rs19;
	and.b32  	%r8, %r7, 255;
	sub.s32 	%r9, %r8, %r6;
	st.param.b32	[func_retval0+0], %r9;
	ret;
}

	// .globl	_Z8Gtolowerc
.visible .func  (.param .b32 func_retval0) _Z8Gtolowerc(
	.param .b32 _Z8Gtolowerc_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<5>;


	ld.param.s8 	%rs1, [_Z8Gtolowerc_param_0];
	add.s16 	%rs2, %rs1, -65;
	and.b16  	%rs3, %rs2, 255;
	setp.lt.u16	%p1, %rs3, 26;
	cvt.u32.u16	%r1, %rs1;
	add.s32 	%r2, %r1, 32;
	cvt.u16.u32	%rs4, %r2;
	selp.b16	%rs5, %rs4, %rs1, %p1;
	cvt.u32.u16	%r3, %rs5;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	// .globl	_Z11GstrcasecmpPKcS0_
.visible .func  (.param .b32 func_retval0) _Z11GstrcasecmpPKcS0_(
	.param .b64 _Z11GstrcasecmpPKcS0__param_0,
	.param .b64 _Z11GstrcasecmpPKcS0__param_1
)
{
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<87>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd3, [_Z11GstrcasecmpPKcS0__param_0];
	ld.param.u64 	%rd2, [_Z11GstrcasecmpPKcS0__param_1];
	mov.u64 	%rd1, 4294967295;

BB8_1:
	ld.u8 	%rs85, [%rd3];
	ld.u8 	%rs2, [%rd2];
	setp.eq.s16	%p1, %rs85, 0;
	mov.u16 	%rs24, 0;
	@%p1 bra 	BB8_2;

	setp.eq.s16	%p2, %rs2, 0;
	mov.u16 	%rs86, 0;
	@%p2 bra 	BB8_27;

	setp.eq.s16	%p3, %rs85, %rs2;
	@%p3 bra 	BB8_6;

	cvt.u32.u16	%r1, %rs85;
	cvt.u32.u16	%r2, %rs2;
	add.s16 	%rs28, %rs85, -65;
	and.b16  	%rs29, %rs28, 255;
	setp.lt.u16	%p4, %rs29, 26;
	add.s32 	%r3, %r1, 32;
	cvt.u16.u32	%rs30, %r3;
	selp.b16	%rs85, %rs30, %rs85, %p4;
	and.b16  	%rs31, %rs85, 255;
	add.s16 	%rs32, %rs2, -65;
	and.b16  	%rs33, %rs32, 255;
	setp.lt.u16	%p5, %rs33, 26;
	add.s32 	%r4, %r2, 32;
	cvt.u16.u32	%rs34, %r4;
	selp.b16	%rs86, %rs34, %rs2, %p5;
	and.b16  	%rs35, %rs86, 255;
	setp.ne.s16	%p6, %rs31, %rs35;
	@%p6 bra 	BB8_27;

BB8_6:
	ld.u8 	%rs85, [%rd3+1];
	ld.u8 	%rs6, [%rd2+1];
	setp.eq.s16	%p7, %rs85, 0;
	mov.u16 	%rs36, 0;
	@%p7 bra 	BB8_7;

	setp.eq.s16	%p8, %rs6, 0;
	mov.u16 	%rs86, 0;
	@%p8 bra 	BB8_27;

	setp.eq.s16	%p9, %rs85, %rs6;
	@%p9 bra 	BB8_11;

	cvt.u32.u16	%r5, %rs85;
	cvt.u32.u16	%r6, %rs6;
	add.s16 	%rs40, %rs85, -65;
	and.b16  	%rs41, %rs40, 255;
	setp.lt.u16	%p10, %rs41, 26;
	add.s32 	%r7, %r5, 32;
	cvt.u16.u32	%rs42, %r7;
	selp.b16	%rs85, %rs42, %rs85, %p10;
	and.b16  	%rs43, %rs85, 255;
	add.s16 	%rs44, %rs6, -65;
	and.b16  	%rs45, %rs44, 255;
	setp.lt.u16	%p11, %rs45, 26;
	add.s32 	%r8, %r6, 32;
	cvt.u16.u32	%rs46, %r8;
	selp.b16	%rs86, %rs46, %rs6, %p11;
	and.b16  	%rs47, %rs86, 255;
	setp.ne.s16	%p12, %rs43, %rs47;
	@%p12 bra 	BB8_27;

BB8_11:
	ld.u8 	%rs85, [%rd3+2];
	ld.u8 	%rs10, [%rd2+2];
	setp.eq.s16	%p13, %rs85, 0;
	mov.u16 	%rs48, 0;
	@%p13 bra 	BB8_12;

	setp.eq.s16	%p14, %rs10, 0;
	mov.u16 	%rs86, 0;
	@%p14 bra 	BB8_27;

	setp.eq.s16	%p15, %rs85, %rs10;
	@%p15 bra 	BB8_16;

	cvt.u32.u16	%r9, %rs85;
	cvt.u32.u16	%r10, %rs10;
	add.s16 	%rs52, %rs85, -65;
	and.b16  	%rs53, %rs52, 255;
	setp.lt.u16	%p16, %rs53, 26;
	add.s32 	%r11, %r9, 32;
	cvt.u16.u32	%rs54, %r11;
	selp.b16	%rs85, %rs54, %rs85, %p16;
	and.b16  	%rs55, %rs85, 255;
	add.s16 	%rs56, %rs10, -65;
	and.b16  	%rs57, %rs56, 255;
	setp.lt.u16	%p17, %rs57, 26;
	add.s32 	%r12, %r10, 32;
	cvt.u16.u32	%rs58, %r12;
	selp.b16	%rs86, %rs58, %rs10, %p17;
	and.b16  	%rs59, %rs86, 255;
	setp.ne.s16	%p18, %rs55, %rs59;
	@%p18 bra 	BB8_27;

BB8_16:
	ld.u8 	%rs85, [%rd3+3];
	ld.u8 	%rs14, [%rd2+3];
	setp.eq.s16	%p19, %rs85, 0;
	mov.u16 	%rs60, 0;
	@%p19 bra 	BB8_17;

	setp.eq.s16	%p20, %rs14, 0;
	mov.u16 	%rs86, 0;
	@%p20 bra 	BB8_27;

	setp.eq.s16	%p21, %rs85, %rs14;
	@%p21 bra 	BB8_21;

	cvt.u32.u16	%r13, %rs85;
	cvt.u32.u16	%r14, %rs14;
	add.s16 	%rs64, %rs85, -65;
	and.b16  	%rs65, %rs64, 255;
	setp.lt.u16	%p22, %rs65, 26;
	add.s32 	%r15, %r13, 32;
	cvt.u16.u32	%rs66, %r15;
	selp.b16	%rs85, %rs66, %rs85, %p22;
	and.b16  	%rs67, %rs85, 255;
	add.s16 	%rs68, %rs14, -65;
	and.b16  	%rs69, %rs68, 255;
	setp.lt.u16	%p23, %rs69, 26;
	add.s32 	%r16, %r14, 32;
	cvt.u16.u32	%rs70, %r16;
	selp.b16	%rs86, %rs70, %rs14, %p23;
	and.b16  	%rs71, %rs86, 255;
	setp.ne.s16	%p24, %rs67, %rs71;
	@%p24 bra 	BB8_27;

BB8_21:
	ld.u8 	%rs85, [%rd3+4];
	ld.u8 	%rs18, [%rd2+4];
	add.s64 	%rd3, %rd3, 5;
	add.s64 	%rd2, %rd2, 5;
	setp.eq.s16	%p25, %rs85, 0;
	mov.u16 	%rs72, 0;
	@%p25 bra 	BB8_22;

	setp.eq.s16	%p26, %rs18, 0;
	mov.u16 	%rs86, 0;
	@%p26 bra 	BB8_27;

	setp.eq.s16	%p27, %rs85, %rs18;
	@%p27 bra 	BB8_26;

	cvt.u32.u16	%r17, %rs85;
	cvt.u32.u16	%r18, %rs18;
	add.s16 	%rs76, %rs85, -65;
	and.b16  	%rs77, %rs76, 255;
	setp.lt.u16	%p28, %rs77, 26;
	add.s32 	%r19, %r17, 32;
	cvt.u16.u32	%rs78, %r19;
	selp.b16	%rs85, %rs78, %rs85, %p28;
	and.b16  	%rs79, %rs85, 255;
	add.s16 	%rs80, %rs18, -65;
	and.b16  	%rs81, %rs80, 255;
	setp.lt.u16	%p29, %rs81, 26;
	add.s32 	%r20, %r18, 32;
	cvt.u16.u32	%rs82, %r20;
	selp.b16	%rs86, %rs82, %rs18, %p29;
	and.b16  	%rs83, %rs86, 255;
	setp.ne.s16	%p30, %rs79, %rs83;
	@%p30 bra 	BB8_27;

BB8_26:
	add.s64 	%rd1, %rd1, -5;
	setp.ne.s64	%p31, %rd1, 0;
	mov.u16 	%rs86, %rs85;
	@%p31 bra 	BB8_1;
	bra.uni 	BB8_27;

BB8_2:
	mov.u16 	%rs85, %rs24;
	mov.u16 	%rs86, %rs2;
	bra.uni 	BB8_27;

BB8_7:
	mov.u16 	%rs85, %rs36;
	mov.u16 	%rs86, %rs6;
	bra.uni 	BB8_27;

BB8_12:
	mov.u16 	%rs85, %rs48;
	mov.u16 	%rs86, %rs10;
	bra.uni 	BB8_27;

BB8_17:
	mov.u16 	%rs85, %rs60;
	mov.u16 	%rs86, %rs14;
	bra.uni 	BB8_27;

BB8_22:
	mov.u16 	%rs85, %rs72;
	mov.u16 	%rs86, %rs18;

BB8_27:
	cvt.u32.u16	%r21, %rs86;
	and.b32  	%r22, %r21, 255;
	cvt.u32.u16	%r23, %rs85;
	and.b32  	%r24, %r23, 255;
	sub.s32 	%r25, %r24, %r22;
	st.param.b32	[func_retval0+0], %r25;
	ret;
}

	// .globl	_Z7GstrcpyPcPKc
.visible .func  (.param .b64 func_retval0) _Z7GstrcpyPcPKc(
	.param .b64 _Z7GstrcpyPcPKc_param_0,
	.param .b64 _Z7GstrcpyPcPKc_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<3>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd5, [_Z7GstrcpyPcPKc_param_0];
	ld.param.u64 	%rd7, [_Z7GstrcpyPcPKc_param_1];
	ld.u8 	%rs1, [%rd7];
	st.u8 	[%rd5], %rs1;
	setp.eq.s16	%p1, %rs1, 0;
	@%p1 bra 	BB9_3;

	mov.u64 	%rd8, %rd5;

BB9_2:
	add.s64 	%rd3, %rd8, 1;
	add.s64 	%rd4, %rd7, 1;
	ld.u8 	%rs2, [%rd7+1];
	st.u8 	[%rd8+1], %rs2;
	setp.ne.s16	%p2, %rs2, 0;
	mov.u64 	%rd7, %rd4;
	mov.u64 	%rd8, %rd3;
	@%p2 bra 	BB9_2;

BB9_3:
	st.param.b64	[func_retval0+0], %rd5;
	ret;
}

	// .globl	_Z8GstrncpyPcPKcy
.visible .func  (.param .b64 func_retval0) _Z8GstrncpyPcPKcy(
	.param .b64 _Z8GstrncpyPcPKcy_param_0,
	.param .b64 _Z8GstrncpyPcPKcy_param_1,
	.param .b64 _Z8GstrncpyPcPKcy_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd9, [_Z8GstrncpyPcPKcy_param_0];
	ld.param.u64 	%rd12, [_Z8GstrncpyPcPKcy_param_1];
	ld.param.u64 	%rd14, [_Z8GstrncpyPcPKcy_param_2];
	mov.u64 	%rd13, %rd9;

BB10_1:
	setp.eq.s64	%p4, %rd14, 0;
	mov.pred 	%p5, 0;
	@%p4 bra 	BB10_3;

	add.s64 	%rd4, %rd12, 1;
	ld.u8 	%rs1, [%rd12];
	add.s64 	%rd5, %rd13, 1;
	st.u8 	[%rd13], %rs1;
	setp.ne.s16	%p5, %rs1, 0;
	mov.u64 	%rd12, %rd4;
	mov.u64 	%rd13, %rd5;

BB10_3:
	add.s64 	%rd14, %rd14, -1;
	@%p5 bra 	BB10_1;

	st.param.b64	[func_retval0+0], %rd9;
	ret;
}

	// .globl	_Z8GstrlcpyPcPKcy
.visible .func  (.param .b64 func_retval0) _Z8GstrlcpyPcPKcy(
	.param .b64 _Z8GstrlcpyPcPKcy_param_0,
	.param .b64 _Z8GstrlcpyPcPKcy_param_1,
	.param .b64 _Z8GstrlcpyPcPKcy_param_2
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<141>;


	ld.param.u64 	%rd77, [_Z8GstrlcpyPcPKcy_param_0];
	ld.param.u64 	%rd43, [_Z8GstrlcpyPcPKcy_param_1];
	ld.param.u64 	%rd79, [_Z8GstrlcpyPcPKcy_param_2];
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd43;
	.param .b64 retval0;
	call.uni (retval0), 
	_Z7GstrlenPKc, 
	(
	param0
	);
	ld.param.b64	%rd1, [retval0+0];
	
	//{
	}// Callseq End 0
	setp.eq.s64	%p1, %rd79, 0;
	@%p1 bra 	BB11_28;

	setp.eq.s64	%p2, %rd43, %rd77;
	@%p2 bra 	BB11_27;

	or.b64  	%rd80, %rd43, %rd77;
	and.b64  	%rd81, %rd80, 3;
	setp.eq.s64	%p3, %rd81, 0;
	add.s64 	%rd82, %rd79, -1;
	setp.lt.u64	%p4, %rd1, %rd79;
	selp.b64	%rd128, %rd1, %rd82, %p4;
	setp.gt.u64	%p5, %rd128, 3;
	and.pred  	%p6, %p3, %p5;
	mov.u64 	%rd44, %rd77;
	@!%p6 bra 	BB11_15;
	bra.uni 	BB11_3;

BB11_3:
	selp.b64	%rd117, %rd1, %rd82, %p4;
	add.s64 	%rd87, %rd117, -4;
	shr.u64 	%rd88, %rd87, 2;
	add.s64 	%rd89, %rd88, 1;
	and.b64  	%rd90, %rd89, 3;
	setp.eq.s64	%p8, %rd90, 0;
	mov.u64 	%rd128, 0;
	@%p8 bra 	BB11_4;

	selp.b64	%rd114, %rd1, %rd82, %p4;
	add.s64 	%rd92, %rd114, -4;
	shr.u64 	%rd93, %rd92, 2;
	add.s64 	%rd94, %rd93, 1;
	and.b64  	%rd95, %rd94, 3;
	setp.eq.s64	%p10, %rd95, 1;
	@%p10 bra 	BB11_6;
	bra.uni 	BB11_7;

BB11_6:
	mov.u64 	%rd115, %rd77;
	bra.uni 	BB11_11;

BB11_4:
	mov.u64 	%rd124, %rd77;
	mov.u64 	%rd125, %rd43;
	mov.u64 	%rd44, %rd128;
	mov.u64 	%rd43, %rd128;
	bra.uni 	BB11_12;

BB11_7:
	selp.b64	%rd111, %rd1, %rd82, %p4;
	add.s64 	%rd97, %rd111, -4;
	shr.u64 	%rd98, %rd97, 2;
	add.s64 	%rd99, %rd98, 1;
	and.b64  	%rd100, %rd99, 3;
	setp.eq.s64	%p12, %rd100, 2;
	@%p12 bra 	BB11_8;
	bra.uni 	BB11_9;

BB11_8:
	mov.u64 	%rd112, %rd77;
	mov.u64 	%rd113, %rd43;
	bra.uni 	BB11_10;

BB11_9:
	add.s64 	%rd113, %rd43, 4;
	ld.u32 	%r1, [%rd43];
	add.s64 	%rd112, %rd77, 4;
	st.u32 	[%rd77], %r1;
	selp.b64	%rd102, %rd1, %rd82, %p4;
	add.s64 	%rd111, %rd102, -4;

BB11_10:
	add.s64 	%rd43, %rd113, 4;
	ld.u32 	%r2, [%rd113];
	add.s64 	%rd115, %rd112, 4;
	st.u32 	[%rd112], %r2;
	add.s64 	%rd114, %rd111, -4;

BB11_11:
	add.s64 	%rd125, %rd43, 4;
	ld.u32 	%r3, [%rd43];
	add.s64 	%rd124, %rd115, 4;
	st.u32 	[%rd115], %r3;
	add.s64 	%rd117, %rd114, -4;
	mov.u64 	%rd128, %rd117;
	mov.u64 	%rd44, %rd124;
	mov.u64 	%rd43, %rd125;

BB11_12:
	setp.lt.u64	%p15, %rd89, 4;
	@%p15 bra 	BB11_15;

	mov.u64 	%rd128, %rd117;

BB11_14:
	ld.u32 	%r4, [%rd125];
	st.u32 	[%rd124], %r4;
	ld.u32 	%r5, [%rd125+4];
	st.u32 	[%rd124+4], %r5;
	ld.u32 	%r6, [%rd125+8];
	st.u32 	[%rd124+8], %r6;
	add.s64 	%rd43, %rd125, 16;
	ld.u32 	%r7, [%rd125+12];
	add.s64 	%rd44, %rd124, 16;
	st.u32 	[%rd124+12], %r7;
	add.s64 	%rd128, %rd128, -16;
	setp.gt.u64	%p16, %rd128, 3;
	mov.u64 	%rd124, %rd44;
	mov.u64 	%rd125, %rd43;
	@%p16 bra 	BB11_14;

BB11_15:
	setp.eq.s64	%p17, %rd128, 0;
	@%p17 bra 	BB11_27;

	and.b64  	%rd48, %rd128, 3;
	setp.eq.s64	%p18, %rd48, 0;
	@%p18 bra 	BB11_17;

	setp.eq.s64	%p19, %rd48, 1;
	@%p19 bra 	BB11_19;
	bra.uni 	BB11_20;

BB11_19:
	mov.u64 	%rd134, %rd128;
	bra.uni 	BB11_24;

BB11_17:
	mov.u64 	%rd138, %rd43;
	mov.u64 	%rd139, %rd44;
	mov.u64 	%rd140, %rd128;
	bra.uni 	BB11_25;

BB11_20:
	setp.eq.s64	%p20, %rd48, 2;
	@%p20 bra 	BB11_21;
	bra.uni 	BB11_22;

BB11_21:
	mov.u64 	%rd129, %rd43;
	mov.u64 	%rd130, %rd44;
	mov.u64 	%rd131, %rd128;
	bra.uni 	BB11_23;

BB11_22:
	add.s64 	%rd131, %rd128, -1;
	add.s64 	%rd129, %rd43, 1;
	ld.u8 	%rs1, [%rd43];
	add.s64 	%rd130, %rd44, 1;
	st.u8 	[%rd44], %rs1;

BB11_23:
	add.s64 	%rd134, %rd131, -1;
	add.s64 	%rd43, %rd129, 1;
	ld.u8 	%rs2, [%rd129];
	add.s64 	%rd44, %rd130, 1;
	st.u8 	[%rd130], %rs2;

BB11_24:
	add.s64 	%rd140, %rd134, -1;
	add.s64 	%rd138, %rd43, 1;
	ld.u8 	%rs3, [%rd43];
	add.s64 	%rd139, %rd44, 1;
	st.u8 	[%rd44], %rs3;

BB11_25:
	setp.lt.u64	%p21, %rd128, 4;
	@%p21 bra 	BB11_27;

BB11_26:
	ld.u8 	%rs4, [%rd138];
	st.u8 	[%rd139], %rs4;
	ld.u8 	%rs5, [%rd138+1];
	st.u8 	[%rd139+1], %rs5;
	ld.u8 	%rs6, [%rd138+2];
	st.u8 	[%rd139+2], %rs6;
	ld.u8 	%rs7, [%rd138+3];
	st.u8 	[%rd139+3], %rs7;
	add.s64 	%rd140, %rd140, -4;
	add.s64 	%rd138, %rd138, 4;
	add.s64 	%rd139, %rd139, 4;
	setp.ne.s64	%p22, %rd140, 0;
	@%p22 bra 	BB11_26;

BB11_27:
	setp.lt.u64	%p23, %rd1, %rd79;
	add.s64 	%rd108, %rd79, -1;
	selp.b64	%rd109, %rd1, %rd108, %p23;
	add.s64 	%rd110, %rd77, %rd109;
	mov.u16 	%rs8, 0;
	st.u8 	[%rd110], %rs8;

BB11_28:
	st.param.b64	[func_retval0+0], %rd1;
	ret;
}

	// .globl	_Z7GmemcpyPvPKvy
.visible .func  (.param .b64 func_retval0) _Z7GmemcpyPvPKvy(
	.param .b64 _Z7GmemcpyPvPKvy_param_0,
	.param .b64 _Z7GmemcpyPvPKvy_param_1,
	.param .b64 _Z7GmemcpyPvPKvy_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<113>;


	ld.param.u64 	%rd74, [_Z7GmemcpyPvPKvy_param_0];
	ld.param.u64 	%rd40, [_Z7GmemcpyPvPKvy_param_1];
	ld.param.u64 	%rd100, [_Z7GmemcpyPvPKvy_param_2];
	setp.eq.s64	%p1, %rd40, %rd74;
	@%p1 bra 	BB12_26;

	or.b64  	%rd77, %rd40, %rd74;
	and.b64  	%rd78, %rd77, 3;
	setp.eq.s64	%p2, %rd78, 0;
	setp.gt.u64	%p3, %rd100, 3;
	and.pred  	%p4, %p2, %p3;
	mov.u64 	%rd41, %rd74;
	@!%p4 bra 	BB12_14;
	bra.uni 	BB12_2;

BB12_2:
	add.s64 	%rd5, %rd100, -4;
	shr.u64 	%rd82, %rd5, 2;
	add.s64 	%rd6, %rd82, 1;
	and.b64  	%rd7, %rd6, 3;
	setp.eq.s64	%p5, %rd7, 0;
	mov.u64 	%rd41, 0;
	@%p5 bra 	BB12_3;

	setp.eq.s64	%p6, %rd7, 1;
	@%p6 bra 	BB12_5;
	bra.uni 	BB12_6;

BB12_5:
	mov.u64 	%rd87, %rd74;
	bra.uni 	BB12_10;

BB12_3:
	mov.u64 	%rd89, %rd100;
	mov.u64 	%rd96, %rd74;
	mov.u64 	%rd97, %rd40;
	mov.u64 	%rd100, %rd41;
	mov.u64 	%rd40, %rd41;
	bra.uni 	BB12_11;

BB12_6:
	setp.eq.s64	%p7, %rd7, 2;
	@%p7 bra 	BB12_7;
	bra.uni 	BB12_8;

BB12_7:
	mov.u64 	%rd5, %rd100;
	mov.u64 	%rd84, %rd74;
	mov.u64 	%rd85, %rd40;
	bra.uni 	BB12_9;

BB12_8:
	add.s64 	%rd85, %rd40, 4;
	ld.u32 	%r1, [%rd40];
	add.s64 	%rd84, %rd74, 4;
	st.u32 	[%rd74], %r1;

BB12_9:
	add.s64 	%rd40, %rd85, 4;
	ld.u32 	%r2, [%rd85];
	add.s64 	%rd87, %rd84, 4;
	st.u32 	[%rd84], %r2;
	add.s64 	%rd100, %rd5, -4;

BB12_10:
	add.s64 	%rd97, %rd40, 4;
	ld.u32 	%r3, [%rd40];
	add.s64 	%rd96, %rd87, 4;
	st.u32 	[%rd87], %r3;
	add.s64 	%rd89, %rd100, -4;
	mov.u64 	%rd100, %rd89;
	mov.u64 	%rd41, %rd96;
	mov.u64 	%rd40, %rd97;

BB12_11:
	setp.lt.u64	%p8, %rd6, 4;
	@%p8 bra 	BB12_14;

	mov.u64 	%rd100, %rd89;

BB12_13:
	ld.u32 	%r4, [%rd97];
	st.u32 	[%rd96], %r4;
	ld.u32 	%r5, [%rd97+4];
	st.u32 	[%rd96+4], %r5;
	ld.u32 	%r6, [%rd97+8];
	st.u32 	[%rd96+8], %r6;
	add.s64 	%rd40, %rd97, 16;
	ld.u32 	%r7, [%rd97+12];
	add.s64 	%rd41, %rd96, 16;
	st.u32 	[%rd96+12], %r7;
	add.s64 	%rd100, %rd100, -16;
	setp.gt.u64	%p9, %rd100, 3;
	mov.u64 	%rd96, %rd41;
	mov.u64 	%rd97, %rd40;
	@%p9 bra 	BB12_13;

BB12_14:
	setp.eq.s64	%p10, %rd100, 0;
	@%p10 bra 	BB12_26;

	and.b64  	%rd45, %rd100, 3;
	setp.eq.s64	%p11, %rd45, 0;
	@%p11 bra 	BB12_16;

	setp.eq.s64	%p12, %rd45, 1;
	@%p12 bra 	BB12_18;
	bra.uni 	BB12_19;

BB12_18:
	mov.u64 	%rd106, %rd100;
	bra.uni 	BB12_23;

BB12_16:
	mov.u64 	%rd110, %rd40;
	mov.u64 	%rd111, %rd41;
	mov.u64 	%rd112, %rd100;
	bra.uni 	BB12_24;

BB12_19:
	setp.eq.s64	%p13, %rd45, 2;
	@%p13 bra 	BB12_20;
	bra.uni 	BB12_21;

BB12_20:
	mov.u64 	%rd101, %rd40;
	mov.u64 	%rd102, %rd41;
	mov.u64 	%rd103, %rd100;
	bra.uni 	BB12_22;

BB12_21:
	add.s64 	%rd103, %rd100, -1;
	add.s64 	%rd101, %rd40, 1;
	ld.u8 	%rs1, [%rd40];
	add.s64 	%rd102, %rd41, 1;
	st.u8 	[%rd41], %rs1;

BB12_22:
	add.s64 	%rd106, %rd103, -1;
	add.s64 	%rd40, %rd101, 1;
	ld.u8 	%rs2, [%rd101];
	add.s64 	%rd41, %rd102, 1;
	st.u8 	[%rd102], %rs2;

BB12_23:
	add.s64 	%rd112, %rd106, -1;
	add.s64 	%rd110, %rd40, 1;
	ld.u8 	%rs3, [%rd40];
	add.s64 	%rd111, %rd41, 1;
	st.u8 	[%rd41], %rs3;

BB12_24:
	setp.lt.u64	%p14, %rd100, 4;
	@%p14 bra 	BB12_26;

BB12_25:
	ld.u8 	%rs4, [%rd110];
	st.u8 	[%rd111], %rs4;
	ld.u8 	%rs5, [%rd110+1];
	st.u8 	[%rd111+1], %rs5;
	ld.u8 	%rs6, [%rd110+2];
	st.u8 	[%rd111+2], %rs6;
	ld.u8 	%rs7, [%rd110+3];
	st.u8 	[%rd111+3], %rs7;
	add.s64 	%rd112, %rd112, -4;
	add.s64 	%rd110, %rd110, 4;
	add.s64 	%rd111, %rd111, 4;
	setp.ne.s64	%p15, %rd112, 0;
	@%p15 bra 	BB12_25;

BB12_26:
	st.param.b64	[func_retval0+0], %rd74;
	ret;
}

	// .globl	_Z7GstrcatPcPKc
.visible .func  (.param .b64 func_retval0) _Z7GstrcatPcPKc(
	.param .b64 _Z7GstrcatPcPKc_param_0,
	.param .b64 _Z7GstrcatPcPKc_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd7, [_Z7GstrcatPcPKc_param_0];
	ld.param.u64 	%rd10, [_Z7GstrcatPcPKc_param_1];
	mov.u64 	%rd9, %rd7;

BB13_1:
	mov.u64 	%rd11, %rd9;
	ld.u8 	%rs1, [%rd11];
	add.s64 	%rd9, %rd11, 1;
	setp.ne.s16	%p1, %rs1, 0;
	@%p1 bra 	BB13_1;

	ld.u8 	%rs2, [%rd10];
	st.u8 	[%rd11], %rs2;
	setp.eq.s16	%p2, %rs2, 0;
	@%p2 bra 	BB13_4;

BB13_3:
	add.s64 	%rd5, %rd11, 1;
	add.s64 	%rd6, %rd10, 1;
	ld.u8 	%rs3, [%rd10+1];
	st.u8 	[%rd11+1], %rs3;
	setp.ne.s16	%p3, %rs3, 0;
	mov.u64 	%rd10, %rd6;
	mov.u64 	%rd11, %rd5;
	@%p3 bra 	BB13_3;

BB13_4:
	st.param.b64	[func_retval0+0], %rd7;
	ret;
}

	// .globl	_Z8GstrncatPcPKcy
.visible .func  (.param .b64 func_retval0) _Z8GstrncatPcPKcy(
	.param .b64 _Z8GstrncatPcPKcy_param_0,
	.param .b64 _Z8GstrncatPcPKcy_param_1,
	.param .b64 _Z8GstrncatPcPKcy_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<4>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd9, [_Z8GstrncatPcPKcy_param_0];
	ld.param.u64 	%rd13, [_Z8GstrncatPcPKcy_param_1];
	ld.param.u64 	%rd15, [_Z8GstrncatPcPKcy_param_2];
	setp.eq.s64	%p1, %rd15, 0;
	@%p1 bra 	BB14_6;

	mov.u64 	%rd12, %rd9;

BB14_2:
	mov.u64 	%rd14, %rd12;
	ld.u8 	%rs1, [%rd14];
	add.s64 	%rd12, %rd14, 1;
	setp.ne.s16	%p2, %rs1, 0;
	@%p2 bra 	BB14_2;

BB14_3:
	mov.u64 	%rd4, %rd14;
	ld.u8 	%rs2, [%rd13];
	st.u8 	[%rd4], %rs2;
	setp.eq.s16	%p3, %rs2, 0;
	@%p3 bra 	BB14_6;

	add.s64 	%rd14, %rd4, 1;
	add.s64 	%rd13, %rd13, 1;
	add.s64 	%rd15, %rd15, -1;
	setp.ne.s64	%p4, %rd15, 0;
	@%p4 bra 	BB14_3;

	mov.u16 	%rs3, 0;
	st.u8 	[%rd4+1], %rs3;

BB14_6:
	st.param.b64	[func_retval0+0], %rd9;
	ret;
}

	// .globl	_Z7GstrcmpPKcS0_
.visible .func  (.param .b32 func_retval0) _Z7GstrcmpPKcS0_(
	.param .b64 _Z7GstrcmpPKcS0__param_0,
	.param .b64 _Z7GstrcmpPKcS0__param_1
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd9, [_Z7GstrcmpPKcS0__param_0];
	ld.param.u64 	%rd8, [_Z7GstrcmpPKcS0__param_1];

BB15_1:
	ld.u8 	%r2, [%rd9];
	add.s64 	%rd3, %rd8, 1;
	ld.u8 	%r3, [%rd8];
	sub.s32 	%r1, %r2, %r3;
	and.b32  	%r4, %r1, 255;
	setp.eq.s32	%p1, %r4, 0;
	setp.eq.s32	%p2, %r2, 0;
	add.s64 	%rd7, %rd9, 1;
	selp.b64	%rd9, %rd7, %rd9, %p1;
	setp.ne.s32	%p3, %r4, 0;
	or.pred  	%p4, %p2, %p3;
	mov.u64 	%rd8, %rd3;
	@!%p4 bra 	BB15_1;
	bra.uni 	BB15_2;

BB15_2:
	cvt.s32.s8 	%r5, %r1;
	st.param.b32	[func_retval0+0], %r5;
	ret;
}

	// .globl	_Z8GstrncmpPKcS0_y
.visible .func  (.param .b32 func_retval0) _Z8GstrncmpPKcS0_y(
	.param .b64 _Z8GstrncmpPKcS0_y_param_0,
	.param .b64 _Z8GstrncmpPKcS0_y_param_1,
	.param .b64 _Z8GstrncmpPKcS0_y_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd11, [_Z8GstrncmpPKcS0_y_param_0];
	ld.param.u64 	%rd12, [_Z8GstrncmpPKcS0_y_param_1];
	ld.param.u64 	%rd13, [_Z8GstrncmpPKcS0_y_param_2];
	setp.eq.s64	%p1, %rd13, 0;
	mov.u32 	%r8, 0;
	@%p1 bra 	BB16_3;

BB16_1:
	ld.s8 	%r5, [%rd11];
	add.s64 	%rd4, %rd12, 1;
	ld.s8 	%r6, [%rd12];
	sub.s32 	%r1, %r5, %r6;
	and.b32  	%r7, %r1, 255;
	setp.eq.s32	%p2, %r7, 0;
	setp.eq.s32	%p3, %r5, 0;
	add.s64 	%rd10, %rd11, 1;
	selp.b64	%rd11, %rd10, %rd11, %p2;
	setp.ne.s32	%p4, %r7, 0;
	or.pred  	%p5, %p3, %p4;
	add.s64 	%rd13, %rd13, -1;
	setp.eq.s64	%p6, %rd13, 0;
	or.pred  	%p7, %p5, %p6;
	mov.u64 	%rd12, %rd4;
	@!%p7 bra 	BB16_1;
	bra.uni 	BB16_2;

BB16_2:
	cvt.s32.s8 	%r8, %r1;

BB16_3:
	st.param.b32	[func_retval0+0], %r8;
	ret;
}

	// .globl	_Z7GstrchrPKci
.visible .func  (.param .b64 func_retval0) _Z7GstrchrPKci(
	.param .b64 _Z7GstrchrPKci_param_0,
	.param .b32 _Z7GstrchrPKci_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd6, [_Z7GstrchrPKci_param_0];
	ld.u8 	%rs5, [%rd6];
	cvt.u32.u16	%r2, %rs5;
	cvt.s32.s8 	%r3, %r2;
	ld.param.s8 	%r1, [_Z7GstrchrPKci_param_1];
	setp.eq.s32	%p1, %r3, %r1;
	@%p1 bra 	BB17_1;
	bra.uni 	BB17_2;

BB17_1:
	mov.u64 	%rd7, %rd6;
	bra.uni 	BB17_4;

BB17_2:
	setp.eq.s16	%p2, %rs5, 0;
	mov.u64 	%rd7, 0;
	@%p2 bra 	BB17_4;

	add.s64 	%rd7, %rd6, 1;
	ld.u8 	%rs5, [%rd6+1];
	cvt.u32.u16	%r4, %rs5;
	cvt.s32.s8 	%r5, %r4;
	setp.ne.s32	%p3, %r5, %r1;
	mov.u64 	%rd6, %rd7;
	@%p3 bra 	BB17_2;

BB17_4:
	st.param.b64	[func_retval0+0], %rd7;
	ret;
}

	// .globl	_Z8GstrrchrPKci
.visible .func  (.param .b64 func_retval0) _Z8GstrrchrPKci(
	.param .b64 _Z8GstrrchrPKci_param_0,
	.param .b32 _Z8GstrrchrPKci_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd5, [_Z8GstrrchrPKci_param_0];
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd5;
	.param .b64 retval0;
	call.uni (retval0), 
	_Z7GstrlenPKc, 
	(
	param0
	);
	ld.param.b64	%rd6, [retval0+0];
	
	//{
	}// Callseq End 1
	add.s64 	%rd8, %rd5, %rd6;
	ld.param.s8 	%r1, [_Z8GstrrchrPKci_param_1];

BB18_1:
	ld.s8 	%r2, [%rd8];
	setp.eq.s32	%p1, %r2, %r1;
	@%p1 bra 	BB18_2;

	add.s64 	%rd8, %rd8, -1;
	mov.u64 	%rd9, 0;
	setp.ge.u64	%p2, %rd8, %rd5;
	@%p2 bra 	BB18_1;
	bra.uni 	BB18_4;

BB18_2:
	mov.u64 	%rd9, %rd8;

BB18_4:
	st.param.b64	[func_retval0+0], %rd9;
	ret;
}

	// .globl	strlen
.visible .func  (.param .b64 func_retval0) strlen(
	.param .b64 strlen_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd3, [strlen_param_0];
	mov.u64 	%rd5, %rd3;

BB19_1:
	mov.u64 	%rd1, %rd5;
	ld.u8 	%rs1, [%rd1];
	add.s64 	%rd5, %rd1, 1;
	setp.ne.s16	%p1, %rs1, 0;
	@%p1 bra 	BB19_1;

	sub.s64 	%rd4, %rd1, %rd3;
	st.param.b64	[func_retval0+0], %rd4;
	ret;
}

	// .globl	_Z8GstrnlenPKcy
.visible .func  (.param .b64 func_retval0) _Z8GstrnlenPKcy(
	.param .b64 _Z8GstrnlenPKcy_param_0,
	.param .b64 _Z8GstrnlenPKcy_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd6, [_Z8GstrnlenPKcy_param_0];
	ld.param.u64 	%rd9, [_Z8GstrnlenPKcy_param_1];
	setp.eq.s64	%p1, %rd9, 0;
	mov.u64 	%rd11, %rd6;
	@%p1 bra 	BB20_4;

	mov.u64 	%rd11, %rd6;

BB20_2:
	add.s64 	%rd9, %rd9, -1;
	ld.u8 	%rs1, [%rd11];
	setp.eq.s16	%p2, %rs1, 0;
	@%p2 bra 	BB20_4;

	add.s64 	%rd11, %rd11, 1;
	setp.ne.s64	%p3, %rd9, 0;
	@%p3 bra 	BB20_2;

BB20_4:
	sub.s64 	%rd8, %rd11, %rd6;
	st.param.b64	[func_retval0+0], %rd8;
	ret;
}

	// .globl	_Z7GstrdupPKc
.visible .func  (.param .b64 func_retval0) _Z7GstrdupPKc(
	.param .b64 _Z7GstrdupPKc_param_0
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<4>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd18, [_Z7GstrdupPKc_param_0];
	setp.eq.s64	%p4, %rd18, 0;
	mov.pred 	%p8, -1;
	@%p4 bra 	BB21_4;

	mov.u64 	%rd16, %rd18;

BB21_2:
	mov.u64 	%rd1, %rd16;
	ld.u8 	%rs1, [%rd1];
	add.s64 	%rd16, %rd1, 1;
	setp.ne.s16	%p5, %rs1, 0;
	@%p5 bra 	BB21_2;

	mov.u64 	%rd12, 1;
	sub.s64 	%rd13, %rd12, %rd18;
	add.s64 	%rd14, %rd13, %rd1;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd14;
	.param .b64 retval0;
	call.uni (retval0), 
	_Z7Gmallocy, 
	(
	param0
	);
	ld.param.b64	%rd17, [retval0+0];
	
	//{
	}// Callseq End 2
	setp.eq.s64	%p8, %rd17, 0;

BB21_4:
	mov.u64 	%rd20, 0;
	@%p8 bra 	BB21_9;

	ld.u8 	%rs2, [%rd18];
	st.u8 	[%rd17], %rs2;
	setp.eq.s16	%p6, %rs2, 0;
	@%p6 bra 	BB21_8;

	mov.u64 	%rd19, %rd17;

BB21_7:
	add.s64 	%rd7, %rd18, 1;
	ld.u8 	%rs3, [%rd18+1];
	add.s64 	%rd8, %rd19, 1;
	st.u8 	[%rd19+1], %rs3;
	setp.ne.s16	%p7, %rs3, 0;
	mov.u64 	%rd18, %rd7;
	mov.u64 	%rd19, %rd8;
	@%p7 bra 	BB21_7;

BB21_8:
	mov.u64 	%rd20, %rd17;

BB21_9:
	st.param.b64	[func_retval0+0], %rd20;
	ret;
}

	// .globl	_Z7GstrspnPKcS0_
.visible .func  (.param .b64 func_retval0) _Z7GstrspnPKcS0_(
	.param .b64 _Z7GstrspnPKcS0__param_0,
	.param .b64 _Z7GstrspnPKcS0__param_1
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<17>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd13, [_Z7GstrspnPKcS0__param_0];
	ld.param.u64 	%rd9, [_Z7GstrspnPKcS0__param_1];
	ld.u8 	%rs3, [%rd13];
	setp.eq.s16	%p1, %rs3, 0;
	mov.u64 	%rd15, 0;
	@%p1 bra 	BB22_8;

	ld.u8 	%rs2, [%rd9];
	mov.u64 	%rd15, 0;

BB22_2:
	setp.eq.s16	%p2, %rs2, 0;
	mov.u16 	%rs16, 0;
	mov.u16 	%rs15, %rs2;
	mov.u64 	%rd14, %rd9;
	@%p2 bra 	BB22_6;

BB22_3:
	setp.eq.s16	%p3, %rs3, %rs15;
	@%p3 bra 	BB22_4;

	add.s64 	%rd4, %rd14, 1;
	ld.u8 	%rs15, [%rd14+1];
	setp.ne.s16	%p4, %rs15, 0;
	mov.u64 	%rd14, %rd4;
	@%p4 bra 	BB22_3;
	bra.uni 	BB22_6;

BB22_4:
	mov.u16 	%rs16, %rs3;

BB22_6:
	and.b16  	%rs13, %rs16, 255;
	setp.eq.s16	%p5, %rs13, 0;
	@%p5 bra 	BB22_8;

	add.s64 	%rd15, %rd15, 1;
	add.s64 	%rd6, %rd13, 1;
	ld.u8 	%rs3, [%rd13+1];
	setp.ne.s16	%p6, %rs3, 0;
	mov.u64 	%rd13, %rd6;
	@%p6 bra 	BB22_2;

BB22_8:
	st.param.b64	[func_retval0+0], %rd15;
	ret;
}

	// .globl	_Z8GstrpbrkPKcS0_
.visible .func  (.param .b64 func_retval0) _Z8GstrpbrkPKcS0_(
	.param .b64 _Z8GstrpbrkPKcS0__param_0,
	.param .b64 _Z8GstrpbrkPKcS0__param_1
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<12>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd10, [_Z8GstrpbrkPKcS0__param_0];
	ld.param.u64 	%rd7, [_Z8GstrpbrkPKcS0__param_1];
	ld.u8 	%rs10, [%rd10];
	setp.eq.s16	%p1, %rs10, 0;
	mov.u64 	%rd12, 0;
	@%p1 bra 	BB23_7;

	ld.u8 	%rs2, [%rd7];

BB23_2:
	setp.eq.s16	%p2, %rs2, 0;
	mov.u16 	%rs11, %rs2;
	mov.u64 	%rd11, %rd7;
	@%p2 bra 	BB23_6;

BB23_3:
	setp.eq.s16	%p3, %rs10, %rs11;
	@%p3 bra 	BB23_4;

	add.s64 	%rd3, %rd11, 1;
	ld.u8 	%rs11, [%rd11+1];
	setp.ne.s16	%p4, %rs11, 0;
	mov.u64 	%rd11, %rd3;
	@%p4 bra 	BB23_3;

BB23_6:
	add.s64 	%rd4, %rd10, 1;
	ld.u8 	%rs10, [%rd10+1];
	setp.ne.s16	%p5, %rs10, 0;
	mov.u64 	%rd10, %rd4;
	@%p5 bra 	BB23_2;
	bra.uni 	BB23_7;

BB23_4:
	mov.u64 	%rd12, %rd10;

BB23_7:
	st.param.b64	[func_retval0+0], %rd12;
	ret;
}

	// .globl	_Z7GstrtokPcPKc
.visible .func  (.param .b64 func_retval0) _Z7GstrtokPcPKc(
	.param .b64 _Z7GstrtokPcPKc_param_0,
	.param .b64 _Z7GstrtokPcPKc_param_1
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<30>;
	.reg .b64 	%rd<36>;


	ld.param.u64 	%rd20, [_Z7GstrtokPcPKc_param_0];
	ld.param.u64 	%rd18, [_Z7GstrtokPcPKc_param_1];
	setp.eq.s64	%p1, %rd20, 0;
	ld.global.u64 	%rd21, [___strtok];
	selp.b64	%rd1, %rd21, %rd20, %p1;
	setp.eq.s64	%p2, %rd1, 0;
	mov.u64 	%rd35, 0;
	@%p2 bra 	BB24_22;

	ld.u8 	%rs3, [%rd1];
	setp.eq.s16	%p3, %rs3, 0;
	mov.u64 	%rd30, 0;
	@%p3 bra 	BB24_9;

	ld.u8 	%rs2, [%rd18];
	mov.u64 	%rd30, 0;
	mov.u64 	%rd28, %rd1;

BB24_3:
	setp.eq.s16	%p4, %rs2, 0;
	mov.u16 	%rs27, 0;
	mov.u16 	%rs26, %rs2;
	mov.u64 	%rd29, %rd18;
	@%p4 bra 	BB24_7;

BB24_4:
	setp.eq.s16	%p5, %rs3, %rs26;
	@%p5 bra 	BB24_5;

	add.s64 	%rd5, %rd29, 1;
	ld.u8 	%rs26, [%rd29+1];
	setp.ne.s16	%p6, %rs26, 0;
	mov.u64 	%rd29, %rd5;
	@%p6 bra 	BB24_4;
	bra.uni 	BB24_7;

BB24_5:
	mov.u16 	%rs27, %rs3;

BB24_7:
	and.b16  	%rs19, %rs27, 255;
	setp.eq.s16	%p7, %rs19, 0;
	@%p7 bra 	BB24_9;

	add.s64 	%rd30, %rd30, 1;
	add.s64 	%rd7, %rd28, 1;
	ld.u8 	%rs3, [%rd28+1];
	setp.ne.s16	%p8, %rs3, 0;
	mov.u64 	%rd28, %rd7;
	@%p8 bra 	BB24_3;

BB24_9:
	add.s64 	%rd35, %rd1, %rd30;
	ld.u8 	%rs28, [%rd35];
	setp.eq.s16	%p9, %rs28, 0;
	@%p9 bra 	BB24_21;

	ld.u8 	%rs9, [%rd18];
	mov.u64 	%rd31, %rd35;

BB24_11:
	setp.eq.s16	%p10, %rs9, 0;
	mov.u16 	%rs29, %rs9;
	mov.u64 	%rd32, %rd18;
	@%p10 bra 	BB24_15;

BB24_12:
	setp.eq.s16	%p11, %rs28, %rs29;
	@%p11 bra 	BB24_13;

	add.s64 	%rd12, %rd32, 1;
	ld.u8 	%rs29, [%rd32+1];
	setp.ne.s16	%p12, %rs29, 0;
	mov.u64 	%rd32, %rd12;
	@%p12 bra 	BB24_12;

BB24_15:
	add.s64 	%rd13, %rd31, 1;
	ld.u8 	%rs28, [%rd31+1];
	mov.u64 	%rd33, 0;
	setp.ne.s16	%p13, %rs28, 0;
	mov.u64 	%rd31, %rd13;
	@%p13 bra 	BB24_11;
	bra.uni 	BB24_16;

BB24_13:
	mov.u64 	%rd33, %rd31;

BB24_16:
	setp.eq.s64	%p14, %rd33, 0;
	mov.u64 	%rd34, 0;
	@%p14 bra 	BB24_20;

	ld.u8 	%rs23, [%rd33];
	setp.eq.s16	%p15, %rs23, 0;
	@%p15 bra 	BB24_18;

	add.s64 	%rd34, %rd33, 1;
	mov.u16 	%rs24, 0;
	st.u8 	[%rd33], %rs24;
	bra.uni 	BB24_20;

BB24_21:
	mov.u64 	%rd35, 0;
	st.global.u64 	[___strtok], %rd35;
	bra.uni 	BB24_22;

BB24_18:
	mov.u64 	%rd34, %rd33;

BB24_20:
	st.global.u64 	[___strtok], %rd34;

BB24_22:
	st.param.b64	[func_retval0+0], %rd35;
	ret;
}

	// .globl	_Z7GstrsepPPcPKc
.visible .func  (.param .b64 func_retval0) _Z7GstrsepPPcPKc(
	.param .b64 _Z7GstrsepPPcPKc_param_0,
	.param .b64 _Z7GstrsepPPcPKc_param_1
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<13>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd10, [_Z7GstrsepPPcPKc_param_0];
	ld.param.u64 	%rd11, [_Z7GstrsepPPcPKc_param_1];
	ld.u64 	%rd1, [%rd10];
	setp.eq.s64	%p1, %rd1, 0;
	mov.u64 	%rd20, 0;
	@%p1 bra 	BB25_11;

	ld.u8 	%rs11, [%rd1];
	setp.eq.s16	%p2, %rs11, 0;
	mov.u64 	%rd19, 0;
	mov.u64 	%rd18, %rd19;
	@%p2 bra 	BB25_8;

	ld.u8 	%rs2, [%rd11];
	mov.u64 	%rd18, %rd1;

BB25_3:
	setp.eq.s16	%p3, %rs2, 0;
	mov.u16 	%rs12, %rs2;
	mov.u64 	%rd17, %rd11;
	@%p3 bra 	BB25_6;

BB25_4:
	setp.eq.s16	%p4, %rs11, %rs12;
	@%p4 bra 	BB25_8;

	add.s64 	%rd4, %rd17, 1;
	ld.u8 	%rs12, [%rd17+1];
	setp.ne.s16	%p5, %rs12, 0;
	mov.u64 	%rd17, %rd4;
	@%p5 bra 	BB25_4;

BB25_6:
	add.s64 	%rd5, %rd18, 1;
	ld.u8 	%rs11, [%rd18+1];
	setp.ne.s16	%p6, %rs11, 0;
	mov.u64 	%rd18, %rd5;
	@%p6 bra 	BB25_3;

	mov.u64 	%rd18, %rd19;

BB25_8:
	setp.eq.s64	%p7, %rd18, 0;
	@%p7 bra 	BB25_10;

	add.s64 	%rd19, %rd18, 1;
	mov.u16 	%rs10, 0;
	st.u8 	[%rd18], %rs10;

BB25_10:
	st.u64 	[%rd10], %rd19;
	mov.u64 	%rd20, %rd1;

BB25_11:
	st.param.b64	[func_retval0+0], %rd20;
	ret;
}

	// .globl	_Z7strswabPKc
.visible .func  (.param .b64 func_retval0) _Z7strswabPKc(
	.param .b64 _Z7strswabPKc_param_0
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd5, [_Z7strswabPKc_param_0];
	setp.eq.s64	%p1, %rd5, 0;
	mov.u64 	%rd9, 0;
	@%p1 bra 	BB26_6;

	ld.u8 	%rs5, [%rd5];
	setp.eq.s16	%p2, %rs5, 0;
	@%p2 bra 	BB26_6;

	add.s64 	%rd8, %rd5, 1;

BB26_3:
	ld.u8 	%rs3, [%rd8];
	setp.eq.s16	%p3, %rs3, 0;
	@%p3 bra 	BB26_5;

	st.u8 	[%rd8+-1], %rs3;
	st.u8 	[%rd8], %rs5;
	add.s64 	%rd3, %rd8, 2;
	ld.u8 	%rs5, [%rd8+1];
	setp.ne.s16	%p4, %rs5, 0;
	mov.u64 	%rd8, %rd3;
	@%p4 bra 	BB26_3;

BB26_5:
	mov.u64 	%rd9, %rd5;

BB26_6:
	st.param.b64	[func_retval0+0], %rd9;
	ret;
}

	// .globl	_Z7GmemsetPviy
.visible .func  (.param .b64 func_retval0) _Z7GmemsetPviy(
	.param .b64 _Z7GmemsetPviy_param_0,
	.param .b32 _Z7GmemsetPviy_param_1,
	.param .b64 _Z7GmemsetPviy_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd18, [_Z7GmemsetPviy_param_0];
	ld.param.u32 	%r1, [_Z7GmemsetPviy_param_1];
	ld.param.u64 	%rd19, [_Z7GmemsetPviy_param_2];
	setp.eq.s64	%p1, %rd19, 0;
	@%p1 bra 	BB27_9;

	cvt.u16.u32	%rs1, %r1;
	and.b64  	%rd1, %rd19, 3;
	setp.eq.s64	%p2, %rd1, 0;
	mov.u64 	%rd26, %rd18;
	mov.u64 	%rd27, %rd19;
	@%p2 bra 	BB27_7;

	setp.eq.s64	%p3, %rd1, 1;
	mov.u64 	%rd22, %rd18;
	mov.u64 	%rd23, %rd19;
	@%p3 bra 	BB27_6;

	setp.eq.s64	%p4, %rd1, 2;
	mov.u64 	%rd20, %rd18;
	mov.u64 	%rd21, %rd19;
	@%p4 bra 	BB27_5;

	add.s64 	%rd21, %rd19, -1;
	add.s64 	%rd20, %rd18, 1;
	st.u8 	[%rd18], %rs1;

BB27_5:
	add.s64 	%rd23, %rd21, -1;
	add.s64 	%rd22, %rd20, 1;
	st.u8 	[%rd20], %rs1;

BB27_6:
	add.s64 	%rd27, %rd23, -1;
	add.s64 	%rd26, %rd22, 1;
	st.u8 	[%rd22], %rs1;

BB27_7:
	setp.lt.u64	%p5, %rd19, 4;
	@%p5 bra 	BB27_9;

BB27_8:
	st.u8 	[%rd26], %rs1;
	st.u8 	[%rd26+1], %rs1;
	st.u8 	[%rd26+2], %rs1;
	st.u8 	[%rd26+3], %rs1;
	add.s64 	%rd27, %rd27, -4;
	add.s64 	%rd26, %rd26, 4;
	setp.ne.s64	%p6, %rd27, 0;
	@%p6 bra 	BB27_8;

BB27_9:
	st.param.b64	[func_retval0+0], %rd18;
	ret;
}

	// .globl	_Z8GmemmovePvPKvy
.visible .func  (.param .b64 func_retval0) _Z8GmemmovePvPKvy(
	.param .b64 _Z8GmemmovePvPKvy_param_0,
	.param .b64 _Z8GmemmovePvPKvy_param_1,
	.param .b64 _Z8GmemmovePvPKvy_param_2
)
{
	.reg .pred 	%p<23>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<152>;


	ld.param.u64 	%rd101, [_Z8GmemmovePvPKvy_param_0];
	ld.param.u64 	%rd40, [_Z8GmemmovePvPKvy_param_1];
	ld.param.u64 	%rd127, [_Z8GmemmovePvPKvy_param_2];
	setp.gt.u64	%p1, %rd101, %rd40;
	@%p1 bra 	BB28_27;
	bra.uni 	BB28_1;

BB28_27:
	setp.eq.s64	%p17, %rd127, 0;
	@%p17 bra 	BB28_38;

	add.s64 	%rd150, %rd101, %rd127;
	add.s64 	%rd149, %rd40, %rd127;
	and.b64  	%rd76, %rd127, 3;
	setp.eq.s64	%p18, %rd76, 0;
	@%p18 bra 	BB28_29;

	setp.eq.s64	%p19, %rd76, 1;
	@%p19 bra 	BB28_31;
	bra.uni 	BB28_32;

BB28_31:
	mov.u64 	%rd143, %rd149;
	mov.u64 	%rd144, %rd150;
	mov.u64 	%rd145, %rd127;
	bra.uni 	BB28_35;

BB28_1:
	setp.eq.s64	%p2, %rd40, %rd101;
	@%p2 bra 	BB28_38;

	or.b64  	%rd104, %rd40, %rd101;
	and.b64  	%rd105, %rd104, 3;
	setp.eq.s64	%p3, %rd105, 0;
	setp.gt.u64	%p4, %rd127, 3;
	and.pred  	%p5, %p3, %p4;
	mov.u64 	%rd41, %rd101;
	@!%p5 bra 	BB28_15;
	bra.uni 	BB28_3;

BB28_3:
	add.s64 	%rd5, %rd127, -4;
	shr.u64 	%rd109, %rd5, 2;
	add.s64 	%rd6, %rd109, 1;
	and.b64  	%rd7, %rd6, 3;
	setp.eq.s64	%p6, %rd7, 0;
	mov.u64 	%rd41, 0;
	@%p6 bra 	BB28_4;

	setp.eq.s64	%p7, %rd7, 1;
	@%p7 bra 	BB28_6;
	bra.uni 	BB28_7;

BB28_6:
	mov.u64 	%rd114, %rd101;
	bra.uni 	BB28_11;

BB28_29:
	mov.u64 	%rd151, %rd127;
	bra.uni 	BB28_36;

BB28_32:
	setp.eq.s64	%p20, %rd76, 2;
	mov.u64 	%rd142, %rd127;
	@%p20 bra 	BB28_34;

	add.s64 	%rd142, %rd127, -1;
	add.s64 	%rd149, %rd40, %rd142;
	ld.u8 	%rs8, [%rd149];
	add.s64 	%rd150, %rd101, %rd142;
	st.u8 	[%rd150], %rs8;

BB28_34:
	add.s64 	%rd145, %rd142, -1;
	add.s64 	%rd143, %rd149, -1;
	ld.u8 	%rs9, [%rd149+-1];
	add.s64 	%rd144, %rd150, -1;
	st.u8 	[%rd150+-1], %rs9;

BB28_35:
	add.s64 	%rd151, %rd145, -1;
	add.s64 	%rd149, %rd143, -1;
	ld.u8 	%rs10, [%rd143+-1];
	add.s64 	%rd150, %rd144, -1;
	st.u8 	[%rd144+-1], %rs10;

BB28_36:
	setp.lt.u64	%p21, %rd127, 4;
	@%p21 bra 	BB28_38;

BB28_37:
	ld.u8 	%rs11, [%rd149+-1];
	st.u8 	[%rd150+-1], %rs11;
	ld.u8 	%rs12, [%rd149+-2];
	st.u8 	[%rd150+-2], %rs12;
	ld.u8 	%rs13, [%rd149+-3];
	st.u8 	[%rd150+-3], %rs13;
	add.s64 	%rd98, %rd149, -4;
	ld.u8 	%rs14, [%rd149+-4];
	add.s64 	%rd99, %rd150, -4;
	st.u8 	[%rd150+-4], %rs14;
	add.s64 	%rd151, %rd151, -4;
	setp.ne.s64	%p22, %rd151, 0;
	mov.u64 	%rd149, %rd98;
	mov.u64 	%rd150, %rd99;
	@%p22 bra 	BB28_37;
	bra.uni 	BB28_38;

BB28_4:
	mov.u64 	%rd116, %rd127;
	mov.u64 	%rd123, %rd101;
	mov.u64 	%rd124, %rd40;
	mov.u64 	%rd127, %rd41;
	mov.u64 	%rd40, %rd41;
	bra.uni 	BB28_12;

BB28_7:
	setp.eq.s64	%p8, %rd7, 2;
	@%p8 bra 	BB28_8;
	bra.uni 	BB28_9;

BB28_8:
	mov.u64 	%rd5, %rd127;
	mov.u64 	%rd111, %rd101;
	mov.u64 	%rd112, %rd40;
	bra.uni 	BB28_10;

BB28_9:
	add.s64 	%rd112, %rd40, 4;
	ld.u32 	%r1, [%rd40];
	add.s64 	%rd111, %rd101, 4;
	st.u32 	[%rd101], %r1;

BB28_10:
	add.s64 	%rd40, %rd112, 4;
	ld.u32 	%r2, [%rd112];
	add.s64 	%rd114, %rd111, 4;
	st.u32 	[%rd111], %r2;
	add.s64 	%rd127, %rd5, -4;

BB28_11:
	add.s64 	%rd124, %rd40, 4;
	ld.u32 	%r3, [%rd40];
	add.s64 	%rd123, %rd114, 4;
	st.u32 	[%rd114], %r3;
	add.s64 	%rd116, %rd127, -4;
	mov.u64 	%rd127, %rd116;
	mov.u64 	%rd41, %rd123;
	mov.u64 	%rd40, %rd124;

BB28_12:
	setp.lt.u64	%p9, %rd6, 4;
	@%p9 bra 	BB28_15;

	mov.u64 	%rd127, %rd116;

BB28_14:
	ld.u32 	%r4, [%rd124];
	st.u32 	[%rd123], %r4;
	ld.u32 	%r5, [%rd124+4];
	st.u32 	[%rd123+4], %r5;
	ld.u32 	%r6, [%rd124+8];
	st.u32 	[%rd123+8], %r6;
	add.s64 	%rd40, %rd124, 16;
	ld.u32 	%r7, [%rd124+12];
	add.s64 	%rd41, %rd123, 16;
	st.u32 	[%rd123+12], %r7;
	add.s64 	%rd127, %rd127, -16;
	setp.gt.u64	%p10, %rd127, 3;
	mov.u64 	%rd123, %rd41;
	mov.u64 	%rd124, %rd40;
	@%p10 bra 	BB28_14;

BB28_15:
	setp.eq.s64	%p11, %rd127, 0;
	@%p11 bra 	BB28_38;

	and.b64  	%rd45, %rd127, 3;
	setp.eq.s64	%p12, %rd45, 0;
	@%p12 bra 	BB28_17;

	setp.eq.s64	%p13, %rd45, 1;
	@%p13 bra 	BB28_19;
	bra.uni 	BB28_20;

BB28_19:
	mov.u64 	%rd133, %rd127;
	bra.uni 	BB28_24;

BB28_17:
	mov.u64 	%rd137, %rd40;
	mov.u64 	%rd138, %rd41;
	mov.u64 	%rd139, %rd127;
	bra.uni 	BB28_25;

BB28_20:
	setp.eq.s64	%p14, %rd45, 2;
	@%p14 bra 	BB28_21;
	bra.uni 	BB28_22;

BB28_21:
	mov.u64 	%rd128, %rd40;
	mov.u64 	%rd129, %rd41;
	mov.u64 	%rd130, %rd127;
	bra.uni 	BB28_23;

BB28_22:
	add.s64 	%rd130, %rd127, -1;
	add.s64 	%rd128, %rd40, 1;
	ld.u8 	%rs1, [%rd40];
	add.s64 	%rd129, %rd41, 1;
	st.u8 	[%rd41], %rs1;

BB28_23:
	add.s64 	%rd133, %rd130, -1;
	add.s64 	%rd40, %rd128, 1;
	ld.u8 	%rs2, [%rd128];
	add.s64 	%rd41, %rd129, 1;
	st.u8 	[%rd129], %rs2;

BB28_24:
	add.s64 	%rd139, %rd133, -1;
	add.s64 	%rd137, %rd40, 1;
	ld.u8 	%rs3, [%rd40];
	add.s64 	%rd138, %rd41, 1;
	st.u8 	[%rd41], %rs3;

BB28_25:
	setp.lt.u64	%p15, %rd127, 4;
	@%p15 bra 	BB28_38;

BB28_26:
	ld.u8 	%rs4, [%rd137];
	st.u8 	[%rd138], %rs4;
	ld.u8 	%rs5, [%rd137+1];
	st.u8 	[%rd138+1], %rs5;
	ld.u8 	%rs6, [%rd137+2];
	st.u8 	[%rd138+2], %rs6;
	ld.u8 	%rs7, [%rd137+3];
	st.u8 	[%rd138+3], %rs7;
	add.s64 	%rd139, %rd139, -4;
	setp.eq.s64	%p16, %rd139, 0;
	add.s64 	%rd137, %rd137, 4;
	add.s64 	%rd138, %rd138, 4;
	@%p16 bra 	BB28_38;
	bra.uni 	BB28_26;

BB28_38:
	st.param.b64	[func_retval0+0], %rd101;
	ret;
}

	// .globl	_Z7GmemcmpPKvS0_y
.visible .func  (.param .b32 func_retval0) _Z7GmemcmpPKvS0_y(
	.param .b64 _Z7GmemcmpPKvS0_y_param_0,
	.param .b64 _Z7GmemcmpPKvS0_y_param_1,
	.param .b64 _Z7GmemcmpPKvS0_y_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd11, [_Z7GmemcmpPKvS0_y_param_0];
	ld.param.u64 	%rd10, [_Z7GmemcmpPKvS0_y_param_1];
	ld.param.u64 	%rd12, [_Z7GmemcmpPKvS0_y_param_2];
	setp.eq.s64	%p1, %rd12, 0;
	mov.u32 	%r6, 0;
	@%p1 bra 	BB29_4;

BB29_1:
	ld.u8 	%rs1, [%rd11];
	ld.u8 	%rs2, [%rd10];
	setp.ne.s16	%p2, %rs1, %rs2;
	@%p2 bra 	BB29_3;

	add.s64 	%rd11, %rd11, 1;
	add.s64 	%rd10, %rd10, 1;
	add.s64 	%rd12, %rd12, -1;
	setp.ne.s64	%p3, %rd12, 0;
	@%p3 bra 	BB29_1;

BB29_3:
	cvt.u32.u16	%r4, %rs1;
	cvt.u32.u16	%r5, %rs2;
	sub.s32 	%r6, %r4, %r5;

BB29_4:
	st.param.b32	[func_retval0+0], %r6;
	ret;
}

	// .globl	_Z8GmemscanPviy
.visible .func  (.param .b64 func_retval0) _Z8GmemscanPviy(
	.param .b64 _Z8GmemscanPviy_param_0,
	.param .b32 _Z8GmemscanPviy_param_1,
	.param .b64 _Z8GmemscanPviy_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd10, [_Z8GmemscanPviy_param_0];
	ld.param.u32 	%r1, [_Z8GmemscanPviy_param_1];
	ld.param.u64 	%rd9, [_Z8GmemscanPviy_param_2];
	setp.eq.s64	%p1, %rd9, 0;
	@%p1 bra 	BB30_3;

BB30_1:
	ld.u8 	%r2, [%rd10];
	setp.eq.s32	%p2, %r2, %r1;
	@%p2 bra 	BB30_3;

	add.s64 	%rd10, %rd10, 1;
	add.s64 	%rd9, %rd9, -1;
	setp.ne.s64	%p3, %rd9, 0;
	@%p3 bra 	BB30_1;

BB30_3:
	st.param.b64	[func_retval0+0], %rd10;
	ret;
}

	// .globl	_Z7GstrstrPKcS0_
.visible .func  (.param .b64 func_retval0) _Z7GstrstrPKcS0_(
	.param .b64 _Z7GstrstrPKcS0__param_0,
	.param .b64 _Z7GstrstrPKcS0__param_1
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd20, [_Z7GstrstrPKcS0__param_0];
	ld.param.u64 	%rd15, [_Z7GstrstrPKcS0__param_1];
	mov.u64 	%rd19, %rd15;

BB31_1:
	mov.u64 	%rd1, %rd19;
	ld.u8 	%rs1, [%rd1];
	add.s64 	%rd19, %rd1, 1;
	setp.ne.s16	%p1, %rs1, 0;
	@%p1 bra 	BB31_1;

	sub.s64 	%rd3, %rd1, %rd15;
	cvt.u32.u64	%r1, %rd3;
	setp.eq.s32	%p2, %r1, 0;
	@%p2 bra 	BB31_12;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.b64	[param0+0], %rd20;
	.param .b64 retval0;
	call.uni (retval0), 
	_Z7GstrlenPKc, 
	(
	param0
	);
	ld.param.b64	%rd17, [retval0+0];
	
	//{
	}// Callseq End 3
	cvt.u32.u64	%r10, %rd17;
	setp.lt.s32	%p3, %r10, %r1;
	mov.u64 	%rd16, 0;
	@%p3 bra 	BB31_11;

	cvt.s64.s32 	%rd4, %rd3;

BB31_5:
	setp.eq.s64	%p4, %rd4, 0;
	mov.u32 	%r11, 0;
	mov.u64 	%rd21, %rd15;
	mov.u64 	%rd22, %rd20;
	mov.u64 	%rd23, %rd4;
	@%p4 bra 	BB31_9;

BB31_6:
	ld.u8 	%rs2, [%rd22];
	cvt.u32.u16	%r4, %rs2;
	ld.u8 	%rs3, [%rd21];
	cvt.u32.u16	%r5, %rs3;
	setp.ne.s16	%p5, %rs2, %rs3;
	@%p5 bra 	BB31_8;

	add.s64 	%rd22, %rd22, 1;
	add.s64 	%rd21, %rd21, 1;
	add.s64 	%rd23, %rd23, -1;
	setp.ne.s64	%p6, %rd23, 0;
	@%p6 bra 	BB31_6;

BB31_8:
	sub.s32 	%r11, %r4, %r5;

BB31_9:
	setp.eq.s32	%p7, %r11, 0;
	@%p7 bra 	BB31_12;

	add.s32 	%r8, %r10, -1;
	add.s64 	%rd20, %rd20, 1;
	setp.gt.s32	%p8, %r10, %r1;
	mov.u32 	%r10, %r8;
	@%p8 bra 	BB31_5;

BB31_11:
	mov.u64 	%rd20, %rd16;

BB31_12:
	st.param.b64	[func_retval0+0], %rd20;
	ret;
}

	// .globl	_Z7GmemchrPKviy
.visible .func  (.param .b64 func_retval0) _Z7GmemchrPKviy(
	.param .b64 _Z7GmemchrPKviy_param_0,
	.param .b32 _Z7GmemchrPKviy_param_1,
	.param .b64 _Z7GmemchrPKviy_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd11, [_Z7GmemchrPKviy_param_0];
	ld.param.u32 	%r1, [_Z7GmemchrPKviy_param_1];
	ld.param.u64 	%rd10, [_Z7GmemchrPKviy_param_2];
	setp.eq.s64	%p1, %rd10, 0;
	mov.u64 	%rd12, 0;
	@%p1 bra 	BB32_5;

	cvt.u16.u32	%rs1, %r1;
	and.b16  	%rs3, %rs1, 255;

BB32_2:
	add.s64 	%rd10, %rd10, -1;
	ld.u8 	%rs2, [%rd11];
	setp.eq.s16	%p2, %rs2, %rs3;
	@%p2 bra 	BB32_3;

	add.s64 	%rd11, %rd11, 1;
	setp.ne.s64	%p3, %rd10, 0;
	@%p3 bra 	BB32_2;
	bra.uni 	BB32_5;

BB32_3:
	mov.u64 	%rd12, %rd11;

BB32_5:
	st.param.b64	[func_retval0+0], %rd12;
	ret;
}

	// .globl	_Z9GstoupperPc
.visible .func _Z9GstoupperPc(
	.param .b64 _Z9GstoupperPc_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd4, [_Z9GstoupperPc_param_0];
	ld.u8 	%rs6, [%rd4];
	setp.eq.s16	%p1, %rs6, 0;
	@%p1 bra 	BB33_4;

BB33_1:
	add.s16 	%rs4, %rs6, -97;
	and.b16  	%rs5, %rs4, 255;
	setp.gt.u16	%p2, %rs5, 25;
	@%p2 bra 	BB33_3;

	cvt.u32.u16	%r1, %rs6;
	add.s32 	%r2, %r1, 224;
	st.u8 	[%rd4], %r2;

BB33_3:
	add.s64 	%rd2, %rd4, 1;
	ld.u8 	%rs6, [%rd4+1];
	setp.ne.s16	%p3, %rs6, 0;
	mov.u64 	%rd4, %rd2;
	@%p3 bra 	BB33_1;

BB33_4:
	ret;
}

	// .globl	_Z9GstolowerPc
.visible .func _Z9GstolowerPc(
	.param .b64 _Z9GstolowerPc_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<3>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd4, [_Z9GstolowerPc_param_0];
	ld.u8 	%rs6, [%rd4];
	setp.eq.s16	%p1, %rs6, 0;
	@%p1 bra 	BB34_4;

BB34_1:
	add.s16 	%rs4, %rs6, -65;
	and.b16  	%rs5, %rs4, 255;
	setp.gt.u16	%p2, %rs5, 25;
	@%p2 bra 	BB34_3;

	cvt.u32.u16	%r1, %rs6;
	add.s32 	%r2, %r1, 32;
	st.u8 	[%rd4], %r2;

BB34_3:
	add.s64 	%rd2, %rd4, 1;
	ld.u8 	%rs6, [%rd4+1];
	setp.ne.s16	%p3, %rs6, 0;
	mov.u64 	%rd4, %rd2;
	@%p3 bra 	BB34_1;

BB34_4:
	ret;
}

	// .globl	_Z8Gtoupperc
.visible .func  (.param .b32 func_retval0) _Z8Gtoupperc(
	.param .b32 _Z8Gtoupperc_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<5>;


	ld.param.s8 	%rs1, [_Z8Gtoupperc_param_0];
	add.s16 	%rs2, %rs1, -97;
	and.b16  	%rs3, %rs2, 255;
	setp.lt.u16	%p1, %rs3, 26;
	cvt.u32.u16	%r1, %rs1;
	add.s32 	%r2, %r1, 224;
	cvt.u16.u32	%rs4, %r2;
	selp.b16	%rs5, %rs4, %rs1, %p1;
	cvt.u32.u16	%r3, %rs5;
	cvt.s32.s8 	%r4, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}


